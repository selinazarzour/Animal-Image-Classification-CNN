# -*- coding: utf-8 -*-
"""Animals-10-Classification (week 2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u_aY_e0jnZ7E0ZzfFApctDQ3kVhq4rP3

## 1. Imports and Setup
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Use GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

"""## 2. Load and Prepare the Animal Dataset

Let's download and load the dataset and display a few random samples from it:
"""

# 1. Define transform: resize, convert to tensor, normalize
# two transforms, one for train, one for test
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(20),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],  # ImageNet stats
                         [0.229, 0.224, 0.225])
])

test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

# RandomHorizontalFlip, RandomRotation, ColorJitter are basic augmentations to artificially increase data variety for the model

# Resize to make all images the same size
# ToTensor because pytorch models work on tensors not PIL images
# Nomalize means the image's pixel will be scaled and shifted from [0,1] to [-1,1] for all 3 channels (RGB)
# Reason behind: Neural nets train faster and more reliably when input features are zero-centered and have similar scale

# 2. Set your dataset path
# data_dir = '/content/drive/MyDrive/U of T/AI Research - Selina X Ayman/archive/raw-img'

import zipfile
import os

# Path to the zip file in your Drive
zip_path = '/content/drive/MyDrive/U of T/AI Research - Selina X Ayman/archive.zip'
# Path to extract the images
extract_path = '/content/animals10-data' # unzipping into the colab machine directly instead of google drive as it's faster

# Unzip if not already done
if not os.path.exists(extract_path):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)

# Set the image folder
data_dir = '/content/animals10-data/archive/raw-img'

"""Explanation:

This block resizes images, normalizes them, loads them from folders, splits into train/test, and creates batch loaders.
"""

# SECOND VERSION SOLUTION
# This guarantees that for each class, the proportion of samples is preserved in both splits

from sklearn.model_selection import StratifiedShuffleSplit
from torch.utils.data import Subset

# Load with default transform (will apply train/test transforms in the next step)
full_dataset = datasets.ImageFolder(root=data_dir)

all_targets = [label for _, label in full_dataset.imgs]
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

indices = list(range(len(full_dataset)))
for train_idx, test_idx in sss.split(indices, all_targets):
    pass

# We will re-wrap these subsets with their respective transforms
class CustomSubset(torch.utils.data.Dataset):
    def __init__(self, subset, transform):
        self.subset = subset
        self.transform = transform

    def __getitem__(self, idx):
        img, label = self.subset[idx]
        img = self.transform(img)
        return img, label

    def __len__(self):
        return len(self.subset)

# Subset without transform first (otherwise double transform is applied)
train_base = Subset(full_dataset, train_idx)
test_base = Subset(full_dataset, test_idx)
train_dataset = CustomSubset(train_base, train_transform)
test_dataset = CustomSubset(test_base, test_transform)

# PyTorch’s ImageFolder applies its transform to all images. To use different transforms for train/test, we wrap each subset in a CustomSubset that applies the right transform per split.

BATCH_SIZE = 32
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

"""## 3. Visualize Some Images"""

def imshow(img):
    img = img / 2 + 0.5  # unnormalize from [-1, 1] to [0, 1]
    npimg = img.numpy() # Converts the tensor to a numpy array, so matplotlib can plot it
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    # PyTorch images are in “channels first” (C, H, W) format (e.g., RGB, 64, 64).
    # Matplotlib expects “channels last” (H, W, C).
    # np.transpose reorders axes so the colors look right.
    plt.axis('off') # Hides axes for a cleaner image grid.

dataiter = iter(train_loader)
images, labels = next(dataiter)

plt.figure(figsize=(8,4)) # size of the image
imshow(torchvision.utils.make_grid(images[:4]))
plt.title('Sample animal images')
plt.show()
print("Labels:", full_dataset.classes)
# labels[:8] gives you the integer class labels from your batch.
# dataset.classes[l] translates integer labels back to human-readable animal names (e.g., "gatto", "pecora", ...).

"""## 4.1 Neural Network for Animal Classification

We'll first build a simple neural network consisting of two fully connected layers and apply this to the animal classification task. Our network will ultimately output a probability distribution over the 10 different classes (0-9).

## 4.2 Define the CNN Model

### Fully connected neural network architecture
To define the architecture of this first fully connected neural network, we'll once again use the the `torch.nn` modules, defining the model using [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html). Note how we first use a [`nn.Flatten`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) layer, which flattens the input so that it can be fed into the model.

In this next block, you'll define the fully connected layers of this simple network.

## Load EfficientNet-B0
"""

# Number of output classes (should match your animal classes)
num_classes = len(full_dataset.classes) # which is 10

# Load EfficientNet-B0, pretrained on ImageNet
model = models.efficientnet_b0(weights="EfficientNet_B0_Weights.IMAGENET1K_V1")
# If above line fails, try: model = models.efficientnet_b0(pretrained=True)

# Replace the last classification layer
model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)

# (Optional: freeze feature extractor for faster convergence)
for param in model.features.parameters():
    param.requires_grad = False

model = model.to(device)
print(model)

"""### Model Metrics and Training Parameters

Before training the model, we need to define components that govern its performance and guide its learning process. These include the loss function, optimizer, and evaluation metrics:

* *Loss function* — This defines how we measure how accurate the model is during training. As was covered in lecture, during training we want to minimize this function, which will "steer" the model in the right direction.
* *Optimizer* — This defines how the model is updated based on the data it sees and its loss function.
* *Metrics* — Here we can define metrics that we want to use to monitor the training and testing steps. In this example, we'll define and take a look at the *accuracy*, the fraction of the images that are correctly classified.

We'll start out by using a stochastic gradient descent (SGD) optimizer initialized with a learning rate of 0.1. Since we are performing a categorical classification task, we'll want to use the [cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).

You'll want to experiment with both the choice of optimizer and learning rate and evaluate how these affect the accuracy of the trained model.
"""

loss_function = nn.CrossEntropyLoss() # It compares the predicted class scores (“logits”) to the true labels
optimizer = optim.Adam(model.parameters(), lr=0.0003) # The optimizer tells the model how to update its weights after seeing how wrong it was (using the loss).

# Why did we use Adam?
# Adam = “Adaptive Moment Estimation”

# It’s a smart optimizer that:
# Adjusts the learning rate individually for every parameter (“weight”) in your network.
# Keeps track of momentum (average direction of past gradients) and scales updates by how variable gradients have been for each parameter.
# Can escape “bad luck” in training better than simple methods.

# What else could we use?
# SGD (Stochastic Gradient Descent):

# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
# Pros: Simpler, sometimes yields better generalization (but can be slower and needs careful tuning).
# Cons: Can get stuck or take a long time to “find the path.”
# RMSprop, Adagrad, Adadelta, etc.:
# Each has its own way of adapting learning rates or smoothing out updates.

# Learning rate controls how big each update is. Too high? You overshoot. Too low? Training is slow.
# Rhe learning rate is 0.001 for Adam

"""### Train the model"""

def train(model, dataloader, loss_function, optimizer, epochs):
    model.train()
    for epoch in range(epochs):
      # Epoch: One complete pass through the entire training dataset.
        total_loss = 0 # for averaging the loss
        correct = 0 # for accuray
        total = 0 # total images processed so far for this epoch
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device) # move the data to the GPU (if available)
            optimizer.zero_grad() # reset gradients to zero
            outputs = model(images)
            loss = loss_function(outputs, labels) # compute the loss
            loss.backward() # calculate the gradient (how much each weight in the model should change to reduce loss)
            optimizer.step() # updates model weights
            total_loss += loss.item() * images.size(0) # adds the loss for this batch
            _, predicted = torch.max(outputs, 1) # get the predicted class for each image in the batch
            total += labels.size(0)
            correct += (predicted == labels).sum().item() # counts how many images were correctly classified
        avg_loss = total_loss / total
        accuracy = correct / total
        train_losses.append(avg_loss) # Store the loss for plotting
        train_accuracies.append(accuracy) # Store the accuracy for plotting
        print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}")

EPOCHS = 25
train_losses = []
train_accuracies = []
val_losses = []
val_accuracies = []
train(model, train_loader, loss_function, optimizer, EPOCHS)

"""As the model trains, the loss and accuracy metrics are displayed. With five epochs and a learning rate of 0.01, this fully connected model should achieve an accuracy of approximatley 0.93 (or 93%) on the training data.

## Plotting Curves
"""

import matplotlib.pyplot as plt

epochs_range = range(1, EPOCHS+1)
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(epochs_range, train_losses, label='Train Loss')
# plt.plot(epochs_range, val_losses, label='Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss vs Epoch')

plt.subplot(1,2,2)
plt.plot(epochs_range, train_accuracies, label='Train Accuracy')
# plt.plot(epochs_range, val_accuracies, label='Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Accuracy vs Epoch')

plt.tight_layout()
plt.show()

"""### Evaluate the CNN Model

Now that we've trained the model, let's evaluate it on the test dataset.
"""

def evaluate(model, dataloader, loss_function):
    model.eval()
    test_loss = 0
    correct = 0
    total = 0
    with torch.no_grad(): # disable gradient calculations as we're not updating weights, just measuring performance. Saves memory and speeds things up.
        for images, labels in dataloader: #  Loop over the test data in batches
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = loss_function(outputs, labels)
            test_loss += loss.item() * images.size(0)
            _, predicted = torch.max(outputs, 1) # Get the predicted class index for each image (the class with the highest score)
            total += labels.size(0) # Add the total number of images together
            correct += (predicted == labels).sum().item() # Count how many were predicted correctly
    avg_loss = test_loss / total
    accuracy = correct / total
    return avg_loss, accuracy

test_loss, test_acc = evaluate(model, test_loader, loss_function)
print('Test Loss:', test_loss)
print('Test Accuracy:', test_acc)

"""You may observe that the accuracy on the test dataset is a little lower than the accuracy on the training dataset. This gap between training accuracy and test accuracy is an example of *overfitting*, when a machine learning model performs worse on new data than on its training data.

### Make predictions with the CNN model

With the model trained, we can use it to make predictions about some images.
"""

# Collect all test predictions, labels, and images for visualization
all_probs = []
all_preds = []
all_true = []
all_imgs = []

model.eval()
with torch.no_grad():
    for images, labels in test_loader:
        outputs = model(images.to(device))
        probs = torch.softmax(outputs, dim=1)
        preds = torch.argmax(probs, dim=1)
        all_probs.append(probs.cpu())
        all_preds.append(preds.cpu())
        all_true.append(labels.cpu())
        all_imgs.append(images.cpu())

all_probs = torch.cat(all_probs)
all_preds = torch.cat(all_preds)
all_true = torch.cat(all_true)
all_imgs = torch.cat(all_imgs)

"""Plot a Grid of Images with Model Predictions"""

translate = {"cane": "dog", "cavallo": "horse", "elefante": "elephant", "farfalla": "butterfly", "gallina": "chicken", "gatto": "cat", "mucca": "cow", "pecora": "sheep", "scoiattolo": "squirrel", "dog": "cane", "cavallo": "horse", "elephant" : "elefante", "butterfly": "farfalla", "chicken": "gallina", "cat": "gatto", "cow": "mucca", "spider": "ragno", "squirrel": "scoiattolo", "ragno": "spider"}

num_display = 10  # Number of images to display
plt.figure(figsize=(20, 4))
for i in range(num_display):
    plt.subplot(2, num_display//2, i+1)
    img = all_imgs[i].numpy().transpose(1,2,0) / 2 + 0.5  # Unnormalize

    # Get Italian class names
    pred_italian = full_dataset.classes[all_preds[i]]
    true_italian = full_dataset.classes[all_true[i]]

    # Translate to English
    pred_label = translate[pred_italian]
    true_label = translate[true_italian]

    plt.imshow(img)
    color = 'blue' if pred_label == true_label else 'red'
    plt.title(f'Pred: {pred_label}\nTrue: {true_label}', color=color)
    plt.axis('off')
plt.suptitle('Animal Image Predictions (Blue = Correct, Red = Wrong)', fontsize=16)
plt.tight_layout()
plt.show()

"""Plot Class Confidence for a Single Image"""

img_idx = 10  # Change this index to visualize a different image
plt.figure(figsize=(3,2))
img = all_imgs[img_idx].numpy().transpose(1,2,0) / 2 + 0.5
plt.imshow(img)
plt.axis('off')

pred_italian = full_dataset.classes[all_preds[img_idx]]
true_italian = full_dataset.classes[all_true[img_idx]]
pred_label = translate[pred_italian]
true_label = translate[true_italian]

plt.title(f'Prediction: {pred_label}\nTrue: {true_label}')
plt.show()

# Plot the class confidence as a bar chart
plt.figure(figsize=(6,3))
plt.bar(range(len(full_dataset.classes)), all_probs[img_idx].numpy())

# Translate x-axis labels to English using the updated translate dictionary
english_labels = [translate[italian] for italian in full_dataset.classes]
plt.xticks(range(len(full_dataset.classes)), english_labels, rotation=45)

plt.title("Class Confidence for This Image")
plt.show()

# --- 5. Plot Confusion Matrix + Metric Curves ---
# Plot accuracy/loss curves
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Test Loss')
plt.legend()
plt.title("Loss over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")

plt.subplot(1,2,2)
plt.plot(train_accuracies, label='Train Accuracy')
plt.plot(val_accuracies, label='Test Accuracy')
plt.legend()
plt.title("Accuracy over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.show()

# Confusion matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(all_true, all_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=english_labels)
fig, ax = plt.subplots(figsize=(8,8))
disp.plot(ax=ax, cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

from sklearn.metrics import classification_report
print(classification_report(all_true, all_preds, target_names=english_labels))

"""## What Does This Say About the Model?

EfficientNet-B0 outperforms basic CNNs on Animals10, showing the power of transfer learning from ImageNet.
Data augmentation (flipping, rotation, color jitter) significantly helped model robustness and reduced overfitting.
Very strong generalization: The train-test accuracy gap is small, so the model isn’t just memorizing training data.
All classes are classified accurately (F1 ≥ 0.89 for all), meaning the network learned distinctive features for each animal type.
Only a handful of confusion cases (e.g., sheep/cow, cat/dog), which could be due to natural visual similarity.
"""